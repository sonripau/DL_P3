{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b0883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a numpy array with size nrows x ncolumns-1. nrows and ncolums are the rows and columns of the dataset\n",
    "#the Date column is skipped (ncolumns-1)\n",
    "def readData(fname):\n",
    "    with open(fname) as f:\n",
    "        fileData = f.read()\n",
    "  \n",
    "    lines = fileData.split(\"\\n\")\n",
    "    header = lines[0].split(\",\")\n",
    "    lines = lines[1:] \n",
    "    #print(header) \n",
    "    #print(\"Data rows: \", len(lines))\n",
    "\n",
    "    rawData = np.zeros((len(lines), len(header)-1)) #skip the Date column\n",
    "\n",
    "    for i, aLine in enumerate(lines):       \n",
    "        splittedLine = aLine.split(\",\")[:]\n",
    "        rawData[i, 0] = splittedLine[0]\n",
    "        rawData[i, 1:] = [float(x) for x in splittedLine[2:]] \n",
    "\n",
    "    return rawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cedb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the train and test data, normalized. It also returns the standard deviation of Weekly_Sales\n",
    "#Each list has a size equal to the number of stores\n",
    "#For each store there is a list of size trainNSaples (testNSamples) x nColums-1 (the store id is skipped)\n",
    "#Columns: Weekly_Sales,Holiday_Flag,Temperature,Fuel_Price,CPI,Unemployment\n",
    "def splitTrainTest(rawData, testPercent):\n",
    "\n",
    "    listStore = np.unique(rawData[:, 0])\n",
    "    trainNSamples = np.zeros(len(listStore))\n",
    "    \n",
    "    for i, storeId in enumerate(listStore):\n",
    "        trainNSamples[i] = np.count_nonzero(rawData[:, 0] == storeId)\n",
    "    trainNSamples = np.floor((1-testPercent) *  trainNSamples)\n",
    "\n",
    "    tmpTrain = np.zeros((int(np.sum(trainNSamples)), len(rawData[0])))\n",
    "\n",
    "    store = -1\n",
    "    counter = 0\n",
    "    counterTrain = 0\n",
    "    storeDict = dict(zip(listStore, trainNSamples))\n",
    "    for i, aLine in enumerate(rawData):\n",
    "        if store != aLine[0]:\n",
    "            store = int(aLine[0])\n",
    "            counter = 0\n",
    "        if(counter < storeDict.get(store)):\n",
    "            tmpTrain[counterTrain] = rawData[i][:]\n",
    "            counterTrain += 1\n",
    "            counter += 1\n",
    "\n",
    "    meanData = tmpTrain.mean(axis=0)\n",
    "    stdData = tmpTrain.std(axis=0)\n",
    "    rawNormData = (rawData - meanData) / stdData\n",
    "\n",
    "    allTrain = list()\n",
    "    allTest = list()\n",
    "    store = -1\n",
    "    counter = 0\n",
    "    for i, aLine in enumerate(rawNormData):\n",
    "        splittedLine = [float(x) for x in aLine[1:]] #skip store id\n",
    "        if store != rawData[i][0]:\n",
    "            if i != 0:\n",
    "                allTrain.append(storeDataTrain)\n",
    "                allTest.append(storeDataTest)\n",
    "            store = int(rawData[i][0])\n",
    "            storeDataTrain = list()\n",
    "            storeDataTest = list()\n",
    "            counter = 0\n",
    "\n",
    "        if(counter < storeDict.get(store)):\n",
    "            storeDataTrain.append(splittedLine)\n",
    "            counter += 1\n",
    "        else:\n",
    "            storeDataTest.append(splittedLine)\n",
    "\n",
    "        if i == len(rawNormData)-1:\n",
    "            allTrain.append(storeDataTrain)\n",
    "            allTest.append(storeDataTest)\n",
    "\n",
    "    return allTrain, allTest, stdData[1] #std of wSales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates a time series given the input and ouput data, the sequence length and the batch size\n",
    "#seqLength is the number of weeks (observations) of data to be used as input\n",
    "#the target will be the weekly sales in 2 weeks\n",
    "def generateTimeSeries(data, wSales, seqLength, batchSize):   \n",
    "    sampling_rate = 1 #keep all the data points \n",
    "    weeksInAdvance = 3\n",
    "    delay = sampling_rate * (seqLength + weeksInAdvance - 1) #the target will be the weekly sales in 2 weeks\n",
    "    \n",
    "    dataset = keras.utils.timeseries_dataset_from_array(\n",
    "        data[:-delay],\n",
    "        targets=wSales[delay:],\n",
    "        sampling_rate=sampling_rate,\n",
    "        sequence_length=seqLength,\n",
    "        shuffle=True,\n",
    "        batch_size=batchSize,\n",
    "        start_index=0)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09807fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTimeSeriesList(theList):\n",
    "    print('list length', len(theList))\n",
    "    print('First element')\n",
    "    input, target = theList[0]\n",
    "    print([float(x) for x in input.numpy().flatten()], [float(x) for x in target.numpy().flatten()])\n",
    "    print('Last element')\n",
    "    input, target = theList[-1]\n",
    "    print([float(x) for x in input.numpy().flatten()], [float(x) for x in target.numpy().flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb5250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the training and test time series\n",
    "#it also returns the standard deviation of Weekly_Sales, and the number of input features\n",
    "def generateTrainTestData(fileName, testPercent, seqLength, batchSize):\n",
    "    rawData = readData(os.path.join(fileName))\n",
    "    allTrain, allTest, stdSales = splitTrainTest(rawData, testPercent)\n",
    "    \n",
    "    for i in range(len(allTrain)):\n",
    "        tmp_train = generateTimeSeries(np.array(allTrain[i]), np.array(allTrain[i])[:,0], seqLength, batchSize)\n",
    "        tmp_test = generateTimeSeries(np.array(allTest[i]), np.array(allTest[i])[:,0], seqLength, batchSize)\n",
    "\n",
    "        if i == 0:\n",
    "            train_dataset = tmp_train\n",
    "            test_dataset = tmp_test\n",
    "        else:\n",
    "            train_dataset = train_dataset.concatenate(tmp_train)\n",
    "            test_dataset = test_dataset.concatenate(tmp_test)\n",
    "    \n",
    "    return train_dataset, test_dataset, stdSales, np.shape(allTrain)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generateTrainTestData(fileName, testPercent, seqLength, batchSize):\n",
    "#trainData, testData: each element comes from keras.utils.timeseries_dataset_from_array, i.e., is a time series\n",
    "#Columns: Weekly_Sales,Holiday_Flag,Temperature,Fuel_Price,CPI,Unemployment\n",
    "\n",
    "testPercent = 0.2\n",
    "seqLength = 2\n",
    "batchSize = 1\n",
    "trainData, testData, stdSales, nFeatures = generateTrainTestData(\"walmart-sales-dataset-of-45stores.csv\",\n",
    "    testPercent, seqLength, batchSize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d89f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
